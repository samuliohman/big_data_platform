## 1. Non-Technical Design

### 1.1 Target Audience and Use Cases
The LiDAR Data Ingestion Pipeline is designed for organizations that generate and analyze high-resolution geospatial data. Typical users include:
- **Municipal Agencies:** For urban planning, infrastructure monitoring, and environmental management.
- **Surveying & Mapping Companies:** For efficient processing and analysis of 3D point-cloud data.
- **Research Institutions:** Enabling studies in forestry, environmental science, and civil engineering.

Tenants use this platform to ingest massive LiDAR datasets and transform them into actionable insights (e.g., digital elevation models, land use change detection) while maintaining strict data isolation.

### 1.2 System Objectives and User Expectations
The system is engineered to:
- **Efficiently Process Massive Datasets:** Read large LAS files, chunk them (e.g., 10,000 points per chunk), and stream the data reliably.
- **Ensure Multi-Tenant Isolation:** Each tenant receives dedicated resources, targeting approximately **5MB/s bandwidth per tenant**, with the overall throughput tuned to 30–50 MB/s on a typical development environment.
- **Support Versatile Ingestion Modes:** Enable both batch and near real-time data ingestion.
- **Provide Robust Monitoring:** Capture performance metrics such as ingestion speed, throughput (MB/s), and processing latency, with detailed logs for later analysis.

### 1.3 Deployment Context and Scalability
- **Development Environment:** Initially deployed on a laptop (e.g., 16GB RAM, 4–8 cores) for development and demonstration, with potential for scale-out on more powerful hardware.
- **Multi-Tenancy Scale:** Designed to support 3–5 concurrent tenants (with current testing on 3 tenants), ensuring each tenant’s data is processed independently via dedicated staging directories and configuration files.

### 1.4 Key Performance Metrics
- **Processing Throughput:** Target of 50-100 MB/s aggregate ingestion rate.
- **Tenant Throughput:** Approximately 5MB/s bandwidth per tenant.
- **Processing Latency:** Maximum of few minutes for files under 1 GB.
- **Scalability:** Linear scaling with additional hardware resources.

---

## 2. Technical Report

### 2.1 Implementation Overview
The core ingestion component is implemented in Python via the `las_chunker.py` script, which:
- **Reads LAS Files:** Efficiently loads large LiDAR datasets.
- **Chunks Data:** Splits data into manageable pieces (10,000 points per chunk) to enable parallel processing.
- **Extracts Core Dimensions:** Retrieves essential fields (X, Y, Z, intensity, classification, point_source_id).
- **Streams Data via Kafka:** Delivers processed chunks to the Kafka topic `"raw-data"` and sends summary metadata to `"raw-data-metadata"`.

- **Multi-tenant Isolation:**
  The system implements robust isolation at multiple levels:
  - **Storage Isolation:** Dedicated staging directories per tenant (`../data/<tenant_id>`)
  - **Processing Isolation:** Tenant-aware processing with ID tracking throughout the pipeline
  - **Database Isolation:** Tenant ID used as a partition key in Cassandra to ensure data separation
  - **Resource Management:** Fair allocation of processing resources across tenants

The ingestion process is orchestrated by the `batch_ingest_manager.py`, which monitors tenant-specific directories for new LAS files and triggers `las_chunker.py` to process them automatically. Additionally, the `las_consumer.py` script consumes data from Kafka and loads it into Cassandra for persistent storage and further analysis. The Cassandra storage component is provided as an external module in the `./External` folder. It was chosen for its high write throughput, horizontal scalability, and robust handling of large-scale geospatial data, making it an ideal solution for the high-volume, time-series data generated by LiDAR ingestion.
  
### 2.2 Technology Choices and Rationale
- **Apache Kafka:**  
  Chosen for its high throughput and fault tolerance, Kafka forms the backbone of the messaging system, decoupling ingestion from downstream processing.
  
- **Python:**  
  Offers simplicity, an extensive ecosystem for data processing, and asynchronous capabilities that enable efficient concurrent handling of large files.
  
- **GZIP Compression:**  
  Employed to reduce network load and meet bandwidth targets (approx. 5MB/s per tenant) without sacrificing performance.
  
- **Asynchronous Processing:**  
  Enhances responsiveness by processing large datasets concurrently, reducing overall ingestion latency.
  
- **Cassandra Storage (External):**  
  Provided in the `./External` folder, Cassandra was selected for its high write throughput, horizontal scalability, and robust support for large-scale geospatial and time-series data. Its flexible schema design ensures efficient ingestion and query performance for the LiDAR data.

### 2.3 Critical Features and Enhancements from Prior Designs
- **Resource and Task Management:**  
  The batch ingestion manager leverages a scheduling algorithm that polls tenant directories, distributes tasks based on tenant priority, and enforces concurrency limits. This ensures fair resource allocation and resilience against overload.
 
  #### Data Flow Diagram
```
tenant-staging-input-dir/<tenant_id>/*.las 
    → File Validation (format, size, naming)
        → Metadata Extraction
            → Data Transformation
                → Batch Formation
                    → Cassandra Write Operations
```
Validation steps ensure data integrity, with corrupted or non-compliant files flagged for review.

- **Logging and Monitoring:**  
Detailed JSON logs capture metrics such as ingestion time, throughput, and errors. These logs facilitate statistical analysis, real-time performance monitoring, and future SLA compliance reports.

- **Error Handling and Recovery:**
  The system implements robust error management strategies:
  - **Chunked Processing:** Breaking large files into manageable chunks minimizes impact of failures
  - **Retry Logic:** Kafka and Cassandra operations employ configurable retry mechanisms
  - **Transaction Logging:** Failed operations are logged for post-processing analysis
  - **Graceful Degradation:** System continues functioning when components fail temporarily

- **Future Integration:**  
The design is modular, allowing future integration with additional messaging systems (e.g., MQTT, Apache Pulsar), storage backends, and enhanced transformation pipelines for quality control and enriched analytics.

### 2.4 Testing and Validation
The system is tested across various scenarios including:
- Small to large file processing (10MB to 2GB)
- Multi-tenant concurrent processing
- Error injection and recovery testing
- Performance benchmarking against target metrics

Tests confirm the system meets its performance targets while maintaining data integrity and tenant isolation. The detailed JSON logs enable comprehensive performance analysis and troubleshooting.

### 2.5 Future Extensions
This design provides a foundation for future enhancements:
- **Stream Processing:** Components designed to adapt to streaming data sources
- **Enhanced Analytics:** The structured data model in Cassandra supports advanced analytics
- **Security Enhancements:** Architecture allows for addition of encryption and access controls
- **Scalability:** Docker-based components can be deployed across multiple nodes for horizontal scaling

*Note: Throughout development, AI assistance (GitHub Copilot) was utilized to accelerate debugging, optimize performance characteristics, and assist with technical documentation generation.*